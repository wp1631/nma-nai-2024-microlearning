{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import contextlib\n",
    "import torch\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    \"\"\"\n",
    "    Returns the sigmoid function, i.e. 1/(1+exp(-X))\n",
    "    \"\"\"\n",
    "\n",
    "    # to avoid runtime warnings, if abs(X) is more than 500, we just cap it there\n",
    "    Y = (\n",
    "        X.copy()\n",
    "    )  # this ensures we don't overwrite entries in X - Python can be a trickster!\n",
    "    toobig = X > 500\n",
    "    toosmall = X < -500\n",
    "    Y[toobig] = 500\n",
    "    Y[toosmall] = -500\n",
    "\n",
    "    return 1.0 / (1.0 + np.exp(-Y))\n",
    "\n",
    "\n",
    "def ReLU(X):\n",
    "    \"\"\"\n",
    "    Returns the ReLU function, i.e. X if X > 0, 0 otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    # to avoid runtime warnings, if abs(X) is more than 500, we just cap it there\n",
    "    Y = (\n",
    "        X.copy()\n",
    "    )  # this ensures we don't overwrite entries in X - Python can be a trickster!\n",
    "    neg = X < 0\n",
    "    Y[neg] = 0\n",
    "\n",
    "    return Y\n",
    "\n",
    "\n",
    "def add_bias(inputs):\n",
    "    \"\"\"\n",
    "    Append an \"always on\" bias unit to some inputs\n",
    "    \"\"\"\n",
    "    return np.append(inputs, np.ones((1, inputs.shape[1])), axis=0)\n",
    "\n",
    "\n",
    "# Creates a random set of batches, returns an array of indices, one for each batch\n",
    "def create_batches(rng, batch_size, num_samples):\n",
    "    \"\"\"\n",
    "    For a given number of samples, returns an array of indices of random batches of the specified size.\n",
    "\n",
    "    If the size of the data is not divisible by the batch size some samples will not be included.\n",
    "    \"\"\"\n",
    "\n",
    "    # determine the total number of batches\n",
    "    num_batches = int(np.floor(num_samples / batch_size))\n",
    "\n",
    "    # get the batches (without replacement)\n",
    "    return rng.choice(\n",
    "        np.arange(num_samples), size=(num_batches, batch_size), replace=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Simple multilayer perceptron model class with one hidden layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_inputs=784,\n",
    "        num_hidden=100,\n",
    "        num_outputs=10,\n",
    "        activation_type=\"sigmoid\",\n",
    "        bias=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a multilayer perceptron with a single hidden layer.\n",
    "\n",
    "        Arguments:\n",
    "        - num_inputs (int, optional): number of input units (i.e., image size)\n",
    "        - num_hidden (int, optional): number of hidden units in the hidden layer\n",
    "        - num_outputs (int, optional): number of output units (i.e., number of\n",
    "          classes)\n",
    "        - activation_type (str, optional): type of activation to use for the hidden\n",
    "          layer ('sigmoid', 'tanh', 'relu' or 'linear')\n",
    "        - bias (bool, optional): if True, each linear layer will have biases in\n",
    "          addition to weights\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_outputs = num_outputs\n",
    "        self.activation_type = activation_type\n",
    "        self.bias = bias  # boolean\n",
    "\n",
    "        # default weights (and biases, if applicable) initialization is used\n",
    "        # see https://github.com/pytorch/pytorch/blob/main/torch/nn/modules/linear.py\n",
    "        self.lin1 = torch.nn.Linear(num_inputs, num_hidden, bias=bias)\n",
    "        self.lin2 = torch.nn.Linear(num_hidden, num_outputs, bias=bias)\n",
    "\n",
    "        self._store_initial_weights_biases()\n",
    "\n",
    "        self._set_activation()  # activation on the hidden layer\n",
    "        self.softmax = torch.nn.Softmax(dim=1)  # activation on the output layer\n",
    "\n",
    "    def _store_initial_weights_biases(self):\n",
    "        \"\"\"\n",
    "        Stores a copy of the network's initial weights and biases.\n",
    "        \"\"\"\n",
    "\n",
    "        self.init_lin1_weight = self.lin1.weight.data.clone()\n",
    "        self.init_lin2_weight = self.lin2.weight.data.clone()\n",
    "        if self.bias:\n",
    "            self.init_lin1_bias = self.lin1.bias.data.clone()\n",
    "            self.init_lin2_bias = self.lin2.bias.data.clone()\n",
    "\n",
    "    def _set_activation(self):\n",
    "        \"\"\"\n",
    "        Sets the activation function used for the hidden layer.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.activation_type.lower() == \"sigmoid\":\n",
    "            self.activation = torch.nn.Sigmoid()  # maps to [0, 1]\n",
    "        elif self.activation_type.lower() == \"tanh\":\n",
    "            self.activation = torch.nn.Tanh()  # maps to [-1, 1]\n",
    "        elif self.activation_type.lower() == \"relu\":\n",
    "            self.activation = torch.nn.ReLU()  # maps to positive\n",
    "        elif self.activation_type.lower() == \"identity\":\n",
    "            self.activation = torch.nn.Identity()  # maps to same\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"{self.activation_type} activation type not recognized. Only \"\n",
    "                \"'sigmoid', 'relu' and 'identity' have been implemented so far.\"\n",
    "            )\n",
    "\n",
    "    def forward(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Runs a forward pass through the network.\n",
    "\n",
    "        Arguments:\n",
    "        - X (torch.Tensor): Batch of input images.\n",
    "        - y (torch.Tensor, optional): Batch of targets. This variable is not used\n",
    "          here. However, it may be needed for other learning rules, to it is\n",
    "          included as an argument here for compatibility.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred (torch.Tensor): Predicted targets.\n",
    "        \"\"\"\n",
    "\n",
    "        l1_input = self.lin1(X.reshape(-1, self.num_inputs))\n",
    "        h = self.activation(l1_input)\n",
    "        l2_input = self.lin2(h)\n",
    "        y_pred = self.softmax(l2_input)\n",
    "        return y_pred\n",
    "\n",
    "    def forward_backprop(self, X):\n",
    "        \"\"\"\n",
    "        Identical to forward(). Should not be overwritten when creating new\n",
    "        child classes to implement other learning rules, as this method is used\n",
    "        to compare the gradients calculated with other learning rules to those\n",
    "        calculated with backprop.\n",
    "        \"\"\"\n",
    "\n",
    "        h = self.activation(self.lin1(X.reshape(-1, self.num_inputs)))\n",
    "        y_pred = self.softmax(self.lin2(h))\n",
    "        return y_pred\n",
    "\n",
    "    def list_parameters(self):\n",
    "        \"\"\"\n",
    "        Returns a list of model names for a gradient dictionary.\n",
    "\n",
    "        Returns:\n",
    "        - params_list (list): List of parameter names.\n",
    "        \"\"\"\n",
    "\n",
    "        params_list = list()\n",
    "\n",
    "        for layer_str in [\"lin1\", \"lin2\"]:\n",
    "            params_list.append(f\"{layer_str}_weight\")\n",
    "            if self.bias:\n",
    "                params_list.append(f\"{layer_str}_bias\")\n",
    "\n",
    "        return params_list\n",
    "\n",
    "    def gather_gradient_dict(self):\n",
    "        \"\"\"\n",
    "        Gathers a gradient dictionary for the model's parameters. Raises a\n",
    "        runtime error if any parameters have no gradients.\n",
    "\n",
    "        Returns:\n",
    "        - gradient_dict (dict): A dictionary of gradients for each parameter.\n",
    "        \"\"\"\n",
    "\n",
    "        params_list = self.list_parameters()\n",
    "\n",
    "        gradient_dict = dict()\n",
    "        for param_name in params_list:\n",
    "            layer_str, param_str = param_name.split(\"_\")  # lin1_parameterName\n",
    "            layer = getattr(self, layer_str)  # self.lin1\n",
    "            grad = getattr(layer, param_str).grad  # slef.lin1.grad\n",
    "            if grad is None:\n",
    "                raise RuntimeError(\"No gradient was computed\")\n",
    "            gradient_dict[param_name] = grad.detach().clone().numpy()\n",
    "\n",
    "        return gradient_dict\n",
    "\n",
    "\n",
    "# Calculate the accuracy of the network on some data\n",
    "def calculate_accuracy(outputs, targets):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy in categorization of some outputs given some targets.\n",
    "    \"\"\"\n",
    "\n",
    "    # binarize the outputs for an easy calculation\n",
    "    categories = (outputs == np.tile(outputs.max(axis=0), (10, 1))).astype(\"float\")\n",
    "\n",
    "    # get the accuracy\n",
    "    accuracy = np.sum(categories * targets) / targets.shape[1]\n",
    "\n",
    "    return accuracy * 100.0\n",
    "\n",
    "\n",
    "def calculate_cosine_similarity(grad_1, grad_2):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between two gradients\n",
    "    \"\"\"\n",
    "    grad_1 = grad_1.flatten()\n",
    "    grad_2 = grad_2.flatten()\n",
    "    return (\n",
    "        np.dot(grad_1, grad_2)\n",
    "        / np.sqrt(np.dot(grad_1, grad_1))\n",
    "        / np.sqrt(np.dot(grad_2, grad_2))\n",
    "    )\n",
    "\n",
    "\n",
    "def calculate_grad_snr(grad, epsilon=1e-3):\n",
    "    \"\"\"\n",
    "    Calculate the average SNR |mean|/std across all parameters in a gradient update\n",
    "    \"\"\"\n",
    "    return np.mean(np.abs(np.mean(grad, axis=0)) / (np.std(grad, axis=0) + epsilon))\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\"\n",
    "    The class for creating and training a two-layer perceptron.\n",
    "    \"\"\"\n",
    "\n",
    "    # The initialization function\n",
    "    def __init__(self, rng, N=100, sigma=1.0, activation=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        The initialization function for the MLP.\n",
    "\n",
    "         - N is the number of hidden units\n",
    "         - sigma is the SD for initializing the weights\n",
    "         - activation is the function to use for unit activity, options are 'sigmoid' and 'ReLU'\n",
    "        \"\"\"\n",
    "\n",
    "        # store the variables for easy access\n",
    "        self.N = N\n",
    "        self.sigma = sigma\n",
    "        self.activation = activation\n",
    "\n",
    "        # initialize the weights\n",
    "        self.W_h = rng.normal(\n",
    "            scale=self.sigma, size=(self.N, 784 + 1)\n",
    "        )  # input-to-hidden weights & bias\n",
    "        self.W_y = rng.normal(\n",
    "            scale=self.sigma, size=(10, self.N + 1)\n",
    "        )  # hidden-to-output weights & bias\n",
    "        self.B = rng.normal(scale=self.sigma, size=(self.N, 10))  # feedback weights\n",
    "\n",
    "    # The non-linear activation function\n",
    "    def activate(self, inputs):\n",
    "        \"\"\"\n",
    "        Pass some inputs through the activation function.\n",
    "        \"\"\"\n",
    "        if self.activation == \"sigmoid\":\n",
    "            Y = sigmoid(inputs)\n",
    "        elif self.activation == \"ReLU\":\n",
    "            Y = ReLU(inputs)\n",
    "        else:\n",
    "            raise Exception(\"Unknown activation function\")\n",
    "        return Y\n",
    "\n",
    "    # The function for performing a forward pass up through the network during inference\n",
    "    def inference(self, rng, inputs, W_h=None, W_y=None, noise=0.0):\n",
    "        \"\"\"\n",
    "        Recognize inputs, i.e. do a forward pass up through the network. If desired, alternative weights\n",
    "        can be provided\n",
    "        \"\"\"\n",
    "\n",
    "        # load the current network weights if no weights given\n",
    "        if W_h is None:\n",
    "            W_h = self.W_h\n",
    "        if W_y is None:\n",
    "            W_y = self.W_y\n",
    "\n",
    "        # calculate the hidden activities\n",
    "        hidden = self.activate(np.dot(W_h, add_bias(inputs)))\n",
    "        if not (noise == 0.0):\n",
    "            hidden += rng.normal(scale=noise, size=hidden.shape)\n",
    "\n",
    "        # calculate the output activities\n",
    "        output = self.activate(np.dot(W_y, add_bias(hidden)))\n",
    "\n",
    "        if not (noise == 0.0):\n",
    "            output += rng.normal(scale=noise, size=output.shape)\n",
    "\n",
    "        return hidden, output\n",
    "\n",
    "    # A function for calculating the derivative of the activation function\n",
    "    def act_deriv(self, activity):\n",
    "        \"\"\"\n",
    "        Calculate the derivative of some activations with respect to the inputs\n",
    "        \"\"\"\n",
    "        if self.activation == \"sigmoid\":\n",
    "            derivative = activity * (1 - activity)\n",
    "        elif self.activation == \"ReLU\":\n",
    "            derivative = 1.0 * (activity > 1)\n",
    "        else:\n",
    "            raise Exception(\"Unknown activation function\")\n",
    "        return derivative\n",
    "\n",
    "    def mse_loss_batch(self, rng, inputs, targets, W_h=None, W_y=None, output=None):\n",
    "        \"\"\"\n",
    "        Calculate the mean-squared error loss on the given targets (average over the batch)\n",
    "        \"\"\"\n",
    "\n",
    "        # do a forward sweep through the network\n",
    "        if output is None:\n",
    "            (hidden, output) = self.inference(rng, inputs, W_h, W_y)\n",
    "        return np.sum((targets - output) ** 2, axis=0)\n",
    "\n",
    "    # The function for calculating the mean-squared error loss\n",
    "    def mse_loss(self, rng, inputs, targets, W_h=None, W_y=None, output=None):\n",
    "        \"\"\"\n",
    "        Calculate the mean-squared error loss on the given targets (average over the batch)\n",
    "        \"\"\"\n",
    "        return np.mean(\n",
    "            self.mse_loss_batch(rng, inputs, targets, W_h=W_h, W_y=W_y, output=output)\n",
    "        )\n",
    "\n",
    "    # function for calculating perturbation updates\n",
    "    def perturb(self, rng, inputs, targets, noise=1.0):\n",
    "        \"\"\"\n",
    "        Calculates the weight updates for perturbation learning, using noise with SD as given\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def node_perturb(self, rng, inputs, targets, noise=1.0):\n",
    "        \"\"\"\n",
    "        Calculates the weight updates for node perturbation learning, using noise with SD as given\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # function for calculating gradient updates\n",
    "    def gradient(self, rng, inputs, targets):\n",
    "        \"\"\"\n",
    "        Calculates the weight updates for gradient descent learning\n",
    "        \"\"\"\n",
    "\n",
    "        # do a forward pass\n",
    "        hidden, output = self.inference(rng, inputs)\n",
    "\n",
    "        # calculate the gradients\n",
    "        error = targets - output\n",
    "        delta_W_h = np.dot(\n",
    "            np.dot(self.W_y[:, :-1].transpose(), error * self.act_deriv(output))\n",
    "            * self.act_deriv(hidden),\n",
    "            add_bias(inputs).transpose(),\n",
    "        )\n",
    "        delta_W_y = np.dot(error * self.act_deriv(output), add_bias(hidden).transpose())\n",
    "\n",
    "        return delta_W_h, delta_W_y\n",
    "\n",
    "    # function for calculating feedback alignment updates\n",
    "    def feedback(self, rng, inputs, targets):\n",
    "        \"\"\"\n",
    "        Calculates the weight updates for feedback alignment learning\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # function for calculating Kolen-Pollack updates\n",
    "    def kolepoll(self, rng, inputs, targets, eta_back=0.01):\n",
    "        \"\"\"\n",
    "        Calculates the weight updates for Kolen-Polack learning\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def return_grad(\n",
    "        self, rng, inputs, targets, algorithm=\"backprop\", eta=0.0, noise=1.0\n",
    "    ):\n",
    "        # calculate the updates for the weights with the appropriate algorithm\n",
    "        if algorithm == \"perturb\":\n",
    "            delta_W_h, delta_W_y = self.perturb(rng, inputs, targets, noise=noise)\n",
    "        elif algorithm == \"node_perturb\":\n",
    "            delta_W_h, delta_W_y = self.node_perturb(rng, inputs, targets, noise=noise)\n",
    "        elif algorithm == \"feedback\":\n",
    "            delta_W_h, delta_W_y = self.feedback(rng, inputs, targets)\n",
    "        elif algorithm == \"kolepoll\":\n",
    "            delta_W_h, delta_W_y = self.kolepoll(rng, inputs, targets, eta_back=eta)\n",
    "        else:\n",
    "            delta_W_h, delta_W_y = self.gradient(rng, inputs, targets)\n",
    "\n",
    "        return delta_W_h, delta_W_y\n",
    "\n",
    "    # function for updating the network\n",
    "    def update(self, rng, inputs, targets, algorithm=\"backprop\", eta=0.01, noise=1.0):\n",
    "        \"\"\"\n",
    "        Updates the synaptic weights (and unit biases) using the given algorithm, options are:\n",
    "\n",
    "        - 'backprop': backpropagation-of-error (default)\n",
    "        - 'perturb' : weight perturbation (use noise with SD as given)\n",
    "        - 'feedback': feedback alignment\n",
    "        - 'kolepoll': Kolen-Pollack\n",
    "        \"\"\"\n",
    "\n",
    "        delta_W_h, delta_W_y = self.return_grad(\n",
    "            rng, inputs, targets, algorithm=algorithm, eta=eta, noise=noise\n",
    "        )\n",
    "\n",
    "        # do the updates\n",
    "        self.W_h += eta * delta_W_h\n",
    "        self.W_y += eta * delta_W_y\n",
    "\n",
    "    # train the network using the update functions\n",
    "    def train(\n",
    "        self,\n",
    "        rng,\n",
    "        images,\n",
    "        labels,\n",
    "        num_epochs,\n",
    "        test_images,\n",
    "        test_labels,\n",
    "        learning_rate=0.01,\n",
    "        batch_size=20,\n",
    "        algorithm=\"backprop\",\n",
    "        noise=1.0,\n",
    "        report=False,\n",
    "        report_rate=10,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Trains the network with algorithm in batches for the given number of epochs on the data provided.\n",
    "\n",
    "        Uses batches with size as indicated by batch_size and given learning rate.\n",
    "\n",
    "        For perturbation methods, uses SD of noise as given.\n",
    "\n",
    "        Categorization accuracy on a test set is also calculated.\n",
    "\n",
    "        Prints a message every report_rate epochs if requested.\n",
    "\n",
    "        Returns an array of the losses achieved at each epoch (and accuracies if test data given).\n",
    "        \"\"\"\n",
    "\n",
    "        # provide an output message\n",
    "        if report:\n",
    "            print(\"Training starting...\")\n",
    "\n",
    "        # make batches from the data\n",
    "        batches = create_batches(rng, batch_size, images.shape[1])\n",
    "\n",
    "        # create arrays to store loss and accuracy values\n",
    "        losses = np.zeros((num_epochs * batches.shape[0],))\n",
    "        accuracy = np.zeros((num_epochs,))\n",
    "        cosine_similarity = np.zeros((num_epochs,))\n",
    "\n",
    "        # estimate the gradient SNR on the test set\n",
    "        grad = np.zeros((test_images.shape[1], *self.W_h.shape))\n",
    "        for t in range(test_images.shape[1]):\n",
    "            inputs = test_images[:, [t]]\n",
    "            targets = test_labels[:, [t]]\n",
    "            grad[t, ...], _ = self.return_grad(\n",
    "                rng, inputs, targets, algorithm=algorithm, eta=0.0, noise=noise\n",
    "            )\n",
    "        snr = calculate_grad_snr(grad)\n",
    "        # run the training for the given number of epochs\n",
    "        update_counter = 0\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            # step through each batch\n",
    "            for b in range(batches.shape[0]):\n",
    "                # get the inputs and targets for this batch\n",
    "                inputs = images[:, batches[b, :]]\n",
    "                targets = labels[:, batches[b, :]]\n",
    "\n",
    "                # calculate the current loss\n",
    "                losses[update_counter] = self.mse_loss(rng, inputs, targets)\n",
    "\n",
    "                # update the weights\n",
    "                self.update(\n",
    "                    rng,\n",
    "                    inputs,\n",
    "                    targets,\n",
    "                    eta=learning_rate,\n",
    "                    algorithm=algorithm,\n",
    "                    noise=noise,\n",
    "                )\n",
    "                update_counter += 1\n",
    "\n",
    "            # calculate the current test accuracy\n",
    "            (testhid, testout) = self.inference(rng, test_images)\n",
    "            accuracy[epoch] = calculate_accuracy(testout, test_labels)\n",
    "            grad_test, _ = self.return_grad(\n",
    "                rng, test_images, test_labels, algorithm=algorithm, eta=0.0, noise=noise\n",
    "            )\n",
    "            grad_bp, _ = self.return_grad(\n",
    "                rng,\n",
    "                test_images,\n",
    "                test_labels,\n",
    "                algorithm=\"backprop\",\n",
    "                eta=0.0,\n",
    "                noise=noise,\n",
    "            )\n",
    "            cosine_similarity[epoch] = calculate_cosine_similarity(grad_test, grad_bp)\n",
    "\n",
    "            # print an output message every 10 epochs\n",
    "            if report and np.mod(epoch + 1, report_rate) == 0:\n",
    "                print(\n",
    "                    \"...completed \",\n",
    "                    epoch + 1,\n",
    "                    \" epochs of training. Current loss: \",\n",
    "                    round(losses[update_counter - 1], 2),\n",
    "                    \".\",\n",
    "                )\n",
    "\n",
    "        # provide an output message\n",
    "        if report:\n",
    "            print(\"Training complete.\")\n",
    "\n",
    "        return (losses, accuracy, cosine_similarity, snr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hebbian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HebbianFunction(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Gradient computing function class for Hebbian learning.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        context, input: torch.Tensor, weight, bias=None, nonlinearity=None, target=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward pass method for the layer. Computes the output of the layer and\n",
    "        stores variables needed for the backward pass.\n",
    "\n",
    "        Arguments:\n",
    "        - context (torch context): context in which variables can be stored for\n",
    "          the backward pass.\n",
    "        - input (torch tensor): input to the layer.\n",
    "        - weight (torch tensor): layer weights.\n",
    "        - bias (torch tensor, optional): layer biases.\n",
    "        - nonlinearity (torch functional, optional): nonlinearity for the layer.\n",
    "        - target (torch tensor, optional): layer target, if applicable.\n",
    "\n",
    "        Returns:\n",
    "        - output (torch tensor): layer output.\n",
    "        \"\"\"\n",
    "\n",
    "        # compute the output for the layer (linear layer with non-linearity)\n",
    "        output = input.mm(weight.t())  # Matrix multiplication\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        if nonlinearity is not None:\n",
    "            output = nonlinearity(output)\n",
    "\n",
    "        # calculate the output to use for the backward pass\n",
    "        output_for_update = (\n",
    "            output if target is None else target\n",
    "        )  # Train on target or not? if not use the actual output\n",
    "\n",
    "        # store variables in the context for the backward pass\n",
    "        context.save_for_backward(input, weight, bias, output_for_update)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(context, grad_output=None):\n",
    "        \"\"\"\n",
    "        Backward pass method for the layer. Computes and returns the gradients for\n",
    "        all variables passed to forward (returning None if not applicable).\n",
    "\n",
    "        Arguments:\n",
    "        - context (torch context): context in which variables can be stored for\n",
    "          the backward pass.\n",
    "        - input (torch tensor): input to the layer.\n",
    "        - weight (torch tensor): layer weights.\n",
    "        - bias (torch tensor, optional): layer biases.\n",
    "        - nonlinearity (torch functional, optional): nonlinearity for the layer.\n",
    "        - target (torch tensor, optional): layer target, if applicable.\n",
    "\n",
    "        Returns:\n",
    "        - grad_input (None): gradients for the input (None, since gradients are not\n",
    "          backpropagated in Hebbian learning).\n",
    "        - grad_weight (torch tensor): gradients for the weights.\n",
    "        - grad_bias (torch tensor or None): gradients for the biases, if they aren't\n",
    "          None.\n",
    "        - grad_nonlinearity (None): gradients for the nonlinearity (None, since\n",
    "          gradients do not apply to the non-linearities).\n",
    "        - grad_target (None): gradients for the targets (None, since\n",
    "          gradients do not apply to the targets).\n",
    "        \"\"\"\n",
    "\n",
    "        input, weight, bias, output_for_update = context.saved_tensors\n",
    "        grad_input = None\n",
    "        grad_weight = None\n",
    "        grad_bias = None\n",
    "        grad_nonlinearity = None\n",
    "        grad_target = None\n",
    "\n",
    "        input_needs_grad = context.needs_input_grad[0]\n",
    "        if input_needs_grad:  # What is pass mean in this case? Why pass?\n",
    "            pass\n",
    "\n",
    "        weight_needs_grad = context.needs_input_grad[1]\n",
    "        if weight_needs_grad:\n",
    "            grad_weight = output_for_update.t().mm(input)\n",
    "            grad_weight = grad_weight / len(input)  # average across batch\n",
    "\n",
    "            # center around 0\n",
    "            grad_weight = grad_weight - grad_weight.mean(\n",
    "                axis=0\n",
    "            )  # center around 0 -> ? Suspect to be one of normalization method\n",
    "\n",
    "            # or apply Oja's rule (not compatible with clamping outputs to the targets!)\n",
    "            #  oja_subtract = output_for_update.pow(2).mm(grad_weight).mean(axis=0)\n",
    "            #  grad_weight = grad_weight - oja_subtract\n",
    "\n",
    "            # take the negative, as the gradient will be subtracted\n",
    "            grad_weight = -grad_weight\n",
    "\n",
    "        if bias is not None:\n",
    "            bias_needs_grad = context.needs_input_grad[2]\n",
    "            if bias_needs_grad:\n",
    "                grad_bias = output_for_update.mean(axis=0)  # average across batch\n",
    "\n",
    "                # center around 0\n",
    "                grad_bias = grad_bias - grad_bias.mean()\n",
    "\n",
    "                # or apply an adaptation of Oja's rule for biases\n",
    "                # (not compatible with clamping outputs to the targets!)\n",
    "                # oja_subtract = (output_for_update.pow(2) * bias).mean(axis=0)\n",
    "                # grad_bias = grad_bias - oja_subtract\n",
    "\n",
    "                # take the negative, as the gradient will be subtracted\n",
    "                grad_bias = -grad_bias\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias, grad_nonlinearity, grad_target\n",
    "\n",
    "\n",
    "class HebbianMultiLayerPerceptron(MultiLayerPerceptron):\n",
    "    \"\"\"\n",
    "    Hebbian multilayer perceptron with one hidden layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, clamp_output=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes a Hebbian multilayer perceptron object\n",
    "\n",
    "        Arguments:\n",
    "        - clamp_output (bool, optional): if True, outputs are clamped to targets,\n",
    "          if available, when computing weight updates.\n",
    "        \"\"\"\n",
    "\n",
    "        self.clamp_output = clamp_output\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def forward(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Runs a forward pass through the network.\n",
    "\n",
    "        Arguments:\n",
    "        - X (torch.Tensor): Batch of input images.\n",
    "        - y (torch.Tensor, optional): Batch of targets, stored for the backward\n",
    "          pass to compute the gradients for the last layer.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred (torch.Tensor): Predicted targets.\n",
    "        \"\"\"\n",
    "\n",
    "        h = HebbianFunction.apply(\n",
    "            X.reshape(-1, self.num_inputs),\n",
    "            self.lin1.weight,\n",
    "            self.lin1.bias,\n",
    "            self.activation,\n",
    "        )\n",
    "\n",
    "        # if targets are provided, they can be used instead of the last layer's\n",
    "        # output to train the last layer.\n",
    "        if y is None or not self.clamp_output:\n",
    "            targets = None\n",
    "        else:\n",
    "            targets = torch.nn.functional.one_hot(\n",
    "                y, num_classes=self.num_outputs\n",
    "            ).float()\n",
    "\n",
    "        y_pred = HebbianFunction.apply(\n",
    "            h, self.lin2.weight, self.lin2.bias, self.softmax, targets\n",
    "        )\n",
    "\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hebbian + Backprop (2 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HebbianBackpropMultiLayerPerceptron(MultiLayerPerceptron):\n",
    "    \"\"\"\n",
    "    Hybrid backprop/Hebbian multilayer perceptron with one hidden layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Runs a forward pass through the network.\n",
    "\n",
    "        Arguments:\n",
    "        - X (torch.Tensor): Batch of input images.\n",
    "        - y (torch.Tensor, optional): Batch of targets, not used here.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred (torch.Tensor): Predicted targets.\n",
    "        \"\"\"\n",
    "\n",
    "        # Hebbian layer\n",
    "        h = HebbianFunction.apply(\n",
    "            X.reshape(-1, self.num_inputs),\n",
    "            self.lin1.weight,\n",
    "            self.lin1.bias,\n",
    "            self.activation,\n",
    "        )\n",
    "\n",
    "        # backprop layer\n",
    "        y_pred = self.softmax(self.lin2(h))\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightPerturbMLP(MLP):\n",
    "    \"\"\"\n",
    "    A multilayer perceptron that is capable of learning through weight perturbation\n",
    "    \"\"\"\n",
    "\n",
    "    def perturb(self, rng, inputs, targets, noise=1.0):\n",
    "        \"\"\"\n",
    "        Calculates the weight updates for perturbation learning, using noise with SD as given\n",
    "        \"\"\"\n",
    "\n",
    "        # get the random perturbations\n",
    "        delta_W_h = rng.normal(scale=noise, size=self.W_h.shape)\n",
    "        delta_W_y = rng.normal(scale=noise, size=self.W_y.shape)\n",
    "\n",
    "        # calculate the loss with and without the perturbations\n",
    "        loss_now = self.mse_loss(rng, inputs, targets)\n",
    "        loss_per = self.mse_loss(\n",
    "            rng, inputs, targets, self.W_h + delta_W_h, self.W_y + delta_W_y\n",
    "        )\n",
    "\n",
    "        # updates\n",
    "        delta_loss = loss_now - loss_per\n",
    "        W_h_update = delta_loss * delta_W_h / noise**2\n",
    "        W_y_update = delta_loss * delta_W_y / noise**2\n",
    "        return W_h_update, W_y_update\n",
    "\n",
    "    def perturb_loss(self, rng, inputs, targets):\n",
    "        \"\"\"\n",
    "        Calculates the perturbation loss for Weight Perturbation MLP.\n",
    "        \"\"\"\n",
    "        delta_W_h, delta_W_y = self.perturb(rng, inputs, targets)\n",
    "        perturb_loss = self.mse_loss(\n",
    "            rng, inputs, targets, W_h=self.W_h + delta_W_h, W_y=self.W_y + delta_W_y\n",
    "        )\n",
    "        return perturb_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodePerturbMLP(MLP):\n",
    "    \"\"\"\n",
    "    A multilayer perceptron that is capable of learning through node perturbation\n",
    "    \"\"\"\n",
    "\n",
    "    def node_perturb(self, rng, inputs, targets, noise=1.0):\n",
    "        \"\"\"\n",
    "        Calculates the weight updates for node perturbation learning, using noise with SD as given\n",
    "        \"\"\"\n",
    "\n",
    "        # get the random perturbations\n",
    "        hidden, output = self.inference(rng, inputs)\n",
    "        hidden_p, output_p = self.inference(rng, inputs, noise=noise)\n",
    "\n",
    "        loss_now = self.mse_loss_batch(rng, inputs, targets, output=output)\n",
    "        loss_per = self.mse_loss_batch(rng, inputs, targets, output=output_p)\n",
    "        delta_loss = loss_now - loss_per\n",
    "\n",
    "        hidden_update = np.mean(\n",
    "            delta_loss\n",
    "            * (\n",
    "                ((hidden_p - hidden) / noise**2)[:, None, :]\n",
    "                * add_bias(inputs)[None, :, :]\n",
    "            ),\n",
    "            axis=2,\n",
    "        )\n",
    "        output_update = np.mean(\n",
    "            delta_loss\n",
    "            * (\n",
    "                ((output_p - output) / noise**2)[:, None, :]\n",
    "                * add_bias(hidden_p)[None, :, :]\n",
    "            ),\n",
    "            axis=2,\n",
    "        )\n",
    "\n",
    "        return (hidden_update, output_update)\n",
    "\n",
    "    def node_perturb_loss(self, rng, inputs, targets, noise=1.0):\n",
    "        \"\"\"\n",
    "        Calculates the node perturbation loss for Node Perturbation MLP.\n",
    "        \"\"\"\n",
    "        delta_W_h, delta_W_y = self.node_perturb(rng, inputs, targets, noise=noise)\n",
    "        node_perturb_loss = self.mse_loss(\n",
    "            rng, inputs, targets, W_h=self.W_h + delta_W_h, W_y=self.W_y + delta_W_y\n",
    "        )\n",
    "        return node_perturb_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackAlignmentMLP(MLP):\n",
    "    \"\"\"\n",
    "    A multilayer perceptron that is capable of learning through the Feedback Alignment algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    # function for calculating feedback alignment updates\n",
    "    def feedback(self, rng, inputs, targets):\n",
    "        \"\"\"\n",
    "        Calculates the weight updates for feedback alignment learning\n",
    "        \"\"\"\n",
    "\n",
    "        # do a forward pass\n",
    "        hidden, output = self.inference(rng, inputs)\n",
    "\n",
    "        # calculate the updates\n",
    "        error = targets - output\n",
    "        delta_W_h = np.dot(\n",
    "            np.dot(self.B, error * self.act_deriv(output)) * self.act_deriv(hidden),\n",
    "            add_bias(inputs).transpose(),\n",
    "        )\n",
    "        delta_W_y = np.dot(error * self.act_deriv(output), add_bias(hidden).transpose())\n",
    "\n",
    "        return delta_W_h, delta_W_y\n",
    "\n",
    "    def feedback_loss(self, rng, inputs, targets):\n",
    "        \"\"\"\n",
    "        Calculates the feedback alignment loss for Feedback Alignment MLP.\n",
    "        \"\"\"\n",
    "        delta_W_h, delta_W_y = self.feedback(rng, inputs, targets)\n",
    "        feedback_loss = self.mse_loss(\n",
    "            rng, inputs, targets, W_h=self.W_h + delta_W_h, W_y=self.W_y + delta_W_y\n",
    "        )\n",
    "        return feedback_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kollen-Pollack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KolenPollackMLP(MLP):\n",
    "    \"\"\"\n",
    "    A multilayer perceptron that is capable of learning through the Kolen-Pollack algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def kolepoll(self, rng, inputs, targets, eta_back=0.01):\n",
    "        \"\"\"\n",
    "        Calculates the weight updates for Kolen-Polack learning\n",
    "        \"\"\"\n",
    "\n",
    "        # do a forward pass\n",
    "        (hidden, output) = self.inference(rng, inputs)\n",
    "\n",
    "        # calculate the updates for the forward weights\n",
    "        error = targets - output\n",
    "        delta_W_h = np.dot(\n",
    "            np.dot(self.B, error * self.act_deriv(output)) * self.act_deriv(hidden),\n",
    "            add_bias(inputs).transpose(),\n",
    "        )\n",
    "        delta_err = np.dot(error * self.act_deriv(output), add_bias(hidden).transpose())\n",
    "        delta_W_y = delta_err - 0.1 * self.W_y\n",
    "\n",
    "        # calculate the updates for the backwards weights and implement them\n",
    "        delta_B = delta_err[:, :-1].transpose() - 0.1 * self.B\n",
    "        self.B += eta_back * delta_B\n",
    "        return (delta_W_h, delta_W_y)\n",
    "\n",
    "    def kolepoll_loss(self, rng, inputs, targets, eta_back=0.01):\n",
    "        \"\"\"\n",
    "        Calculates the Kolen-Pollack loss for Kolen-Pollack MLP.\n",
    "        \"\"\"\n",
    "        delta_W_h, delta_W_y = self.kolepoll(rng, inputs, targets, eta_back=eta_back)\n",
    "        kolepoll_loss = self.mse_loss(\n",
    "            rng, inputs, targets, W_h=self.W_h + delta_W_h, W_y=self.W_y + delta_W_y\n",
    "        )\n",
    "        return kolepoll_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
